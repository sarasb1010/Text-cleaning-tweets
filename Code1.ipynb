{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The code cleans up raw text from an Excel file by removing stopwords, URLs, usernames, short words, and converting each word to its root form"
      ],
      "metadata": {
        "id": "5AbbS1bznGLH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v3jVbwTnA3n"
      },
      "outputs": [],
      "source": [
        "#code 1\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from spacy.lang.en import English\n",
        "import os\n",
        "\n",
        "\n",
        "# Load spaCy English model\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "parser = English()\n",
        "sp_stop = sp.Defaults.stop_words\n",
        "\n",
        "# Tokenization\n",
        "def tokenize(text):\n",
        "    lda_tokens = []\n",
        "    tokens = parser(text)\n",
        "    for token in tokens:\n",
        "        if token.orth_.isspace():\n",
        "            continue\n",
        "        elif token.like_url or token.orth_.startswith('@'):\n",
        "            continue\n",
        "        else:\n",
        "            lda_tokens.append(token.lower_)\n",
        "    return lda_tokens\n",
        "\n",
        "# Lemmatization\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def get_lemma(word):\n",
        "    lemma = wn.morphy(word)\n",
        "    return word if lemma is None else lemma\n",
        "\n",
        "# Text cleaning\n",
        "def prepare_text_for_lda(text):\n",
        "    tokens = tokenize(text)\n",
        "    tokens = [token for token in tokens if len(token) > 4]\n",
        "    tokens = [token for token in tokens if token not in sp_stop]\n",
        "    tokens = [get_lemma(token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Load your Excel file (adjust filename here)\n",
        "df = pd.read_excel('data_processed_v1.xlsx')  # ðŸ‘ˆ replace with your file\n",
        "\n",
        "# Prepare cleaned tokens\n",
        "cleaned_texts = []\n",
        "for line in df['text']:\n",
        "    tokens = prepare_text_for_lda(str(line))\n",
        "    cleaned_texts.append(' '.join(tokens))\n",
        "\n",
        "df['cleaned_text'] = cleaned_texts\n",
        "\n",
        "# Save the cleaned dataframe as a new Excel file\n",
        "df.to_excel('cleaned_output.xlsx', index=False)\n"
      ]
    }
  ]
}