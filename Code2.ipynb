#Code 2 
pip install ftfy
import spacy
import pandas as pd
from spacy.lang.en import English
import nltk
import re
import ftfy  # <-- install this if not: pip install ftfy

# Load spaCy English model
sp = spacy.load('en_core_web_sm')
parser = English()

# Keywords and protected mentions
keywords = [
    "uber pool", "uberpool", "uber_pool", "uber carpool",
    "lyft line", "lyft shared", "lyftline", "lyft_line",
    "lyftshared", "lyft_shared", "lyft share", "lyftshare",
    "lyft carpool",
    "uber comfort", "ubercomfort", "uber_comfort",
    "uber black", "uberblack", "uber_black",
    "uber x", "uberx", "uber_x",
    "lyft xl", "lyftxl", "lyft_xl",
    "lyft lux", "lyftlux", "lyft_lux",
    "lyft black", "lyftblack", "lyft_black",
    "lyft black xl", "lyftblackxl", "lyft_black_xl",
    "ola Kaali Peeli", "olaKaaliPeeli", "KaaliPeeli",
    "Blablacar carpool", "sride",
    "Ibibo ryde", "ibiboryde", "ibibo_ryde",
    "meru carpool", "merucarpool", "meru_carpool",
    "ola share", "olashare", "ola_share",
    "ola carpool", "ola pool"
]
keyword_set = set(k.lower() for k in keywords)

protected_mentions = ['uber', 'lyft', 'ola', 'meru', 'ibibo']

def protect_keywords(text):
    mapping = {}
    i = 0
    for key in sorted(keyword_set, key=lambda x: -len(x)):  # Longer first
        if key in text:
            marker = f"__KEYWORD{i}__"
            text = text.replace(key, marker)
            mapping[marker] = key
            i += 1
    return text, mapping

def restore_keywords(text, mapping):
    for marker, key in mapping.items():
        text = text.replace(marker, key)
    return text

def tokenize(text):
    tokens = []
    doc = parser(text)
    for token in doc:
        if token.orth_.isspace():
            continue
        elif token.like_url:
            continue
        elif token.text.startswith('@'):
            mention = token.text[1:].lower()
            if any(company in mention for company in protected_mentions):
                tokens.append(token.text)
            else:
                continue
        else:
            tokens.append(token.text)
    return tokens

def prepare_text(text):
    text = ftfy.fix_text(text)  # Fix encoding issues

    text = text.lower()

    # Protect keywords
    text, mapping = protect_keywords(text)

    # Tokenize
    tokens = tokenize(text)

    # Build cleaned text
    cleaned_text = ' '.join(tokens)

    # Restore protected keywords
    cleaned_text = restore_keywords(cleaned_text, mapping)

    return cleaned_text

# Load your Excel
df = pd.read_excel('data_processed_v1.xlsx')

# Process each text
cleaned_texts = []
for line in df['text']:
    cleaned_text = prepare_text(str(line))
    cleaned_texts.append(cleaned_text)

df['cleaned_text'] = cleaned_texts

# Save to new file
df.to_excel('plzzz.xlsx', index=False)
